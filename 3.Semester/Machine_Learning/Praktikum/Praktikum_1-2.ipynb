{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 1: Data Preprocessing - California Housing Datasets. \n",
    "\n",
    "Um sicherzustellen, dass alle notwendigen Python-Dependencies zur Verfügung stehen, arbeiten wir in diesem Praktikum in einem Docker Container. \n",
    "\n",
    "**Vorgehen:**\n",
    "- Laden Sie das Jupyter Notebook von Moodle in einen ML1-Praktikums-Ordner in Ihrem User-Ordner. \n",
    "- Geben Sie im Terminal ein: \"docker_start_ml1_gpu\". Dadurch wird ein Skript aufgerufen, das einen Docker-Container für Sie baut. Das Terminal bleibt offen, wechselt nun allerdings in einen Terminal des Docker-Containers.\n",
    "- Öffnen Sie den angegebenen localhost:8888-Link mit Token im Browser. Ein Jupyter Notebook sollte sich öffnen. Unter dem Ordner \"host\" finden Sie Ihr Homedirectory und auch das Jupyter-Notebook für das Praktikum. \n",
    "- Zum Ende des Praktikums speichern Sie Ihre Fortschritte, und beenden dann den Docker Container mit der Eingabe von \"exit\" im Terminal. \n",
    "\n",
    "\n",
    "**Regeln für das Praktikum:**\n",
    "- Sie können die Aufgaben alleine oder zu zweit bearbeiten.\n",
    "- Sie dürfen in Vorlesungs-Skripten nachlesen\n",
    "- Sollten Fragen auftauchen, oder die Verwendung von Klassen unklar sein, schlagen Sie  die Dokumentation online nach\n",
    "- Befolgen Sie einfach den Anweisungen im Skript; manchmal wird im Markdown Text eine Frage gestellt; tragen Sie Ihre Antwort in der nächsten Markdown-Zelle bei #TODO ein!\n",
    "\n",
    "**Hinweis:**\n",
    "Wir verwenden zum Aufbereiten des Datensets die [pandas API](https://pandas.pydata.org/docs/reference/)\n",
    "Zum Visualisieren von Daten können Sie Matplotlib oder Seaborne verwenden, die Sie bereits aus Data Science kennen, oder pandas-Funktionen.\n",
    "Zum Trainieren von Modellen verwenden wir in dieser Vorlesung die [Scikit Learn API](https://scikit-learn.org/stable/)\n",
    "\n",
    "\n",
    "Wir haben in der Vorlesung das **California Housing Dataset** besprochen: Es enthält die mittleren Preise für Häuser in Bezirken von Californien zusammen mit den geografischen Daten wie Längen- und Breitengrad und Nähe zum Meer, Zensus-Daten wie den Einwohnerzahlen in jedem Bezirk, der Gesamtanzahl von Räumen und Haushalten, sowie dem mittleren Einkommen in den Bezirken. \n",
    "\n",
    "**Ziel: Wir wollen mit Hilfe dieser Daten herausfinden, wie man den mittleren Hauspreis von den anderen gegebenen Daten ableiten kann.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Schritt: Datenvisualisierungen\n",
    "\n",
    "Der erste Schritt wenn man mit Daten arbeitet ist, diese so gut wie möglich kennen zu lernen und zu visualisieren. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie die vorbefüllten Zellen aus, welche die Daten von \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz\" in einen Unterordner \"housing\" von \"datasets\" im ML1-Prakikumsordner speichern, in dem sich auch dieses Notebook befindet. Dann werden die Daten geladen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der notwendigen Dependencies\n",
    "import os\n",
    "# zum Entpacken von Dateien:\n",
    "import tarfile\n",
    "# zum Herunterladen von Dateien:\n",
    "import urllib.request\n",
    "# zum Bearbeiten von \"DataFrames\"\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "\n",
    "# Funktion zum Herunterladen der Daten\n",
    "\"\"\"Optional Arguments: housing_url=HOUSING_URL, housing_path=HOUSING_PATH\"\"\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Lade der Daten\n",
    "\"\"\"Optional Argument: housing_path=HOUSING_PATH\"\"\"\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH): \n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holen Sie sich Informationen über den Datensatz \"housing\", indem Sie die Anweisungen der Kommentare unten befolgen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sehen Sie sich die ersten Zeilen der Tabelle mit .head() an.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was ist ein Instance des Datensatzes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holen Sie sich die grundlegenden Infos über die Daten mit .info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was sind die Features und das Label der Daten? Notieren Sie diese: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features: #TODO\n",
    "\n",
    "Label: #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Features sind nicht numerisch? Sind diese ordinal oder nominal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lassen Sie sich die wichtigsten statistischen Daten zu den einzelnen Features ausgeben\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Werfen Sie einen Blick auf den Mittelwert von total_rooms und total_bedrooms. Kann es sich hierbei um die Anzahl von Räumen je Haus handeln? Was beschreiben die Werte wohl? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nun geht es an die Visualisierungen in Form von Histogrammen, also die Unterteilung der Daten in diskrete Klassen und Anzeige der Häufigkeit jeder Klasse. Lassen Sie Sich Histogramme aller Features ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeige von Histogrammen der Features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesen Sie von den Histograms ab, wo (longitude und latitude) die meisten districts sind. Googlen Sie, wo sich dieser Ort befindet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort: #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes werten wir **Pearson's Korrelationsmatrix** aus. \n",
    "Berechnen Sie die Korrelationsmatrix in der ersten Zelle und lassen Sie sie ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnen der Korrelationsmatrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisieren Sie die Absolutbeträge der Korrelationsmatrix mit .matshow() in der nächsten Zelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Korrelationsmatrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Spalte der Korrelationsmatrix ist die wichtige für unser Ziel, median_house_value zu prognostizieren? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lassen Sie sich diese separat mit absteigenden Werten ausgeben in der nächsten Zelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welches Feature ist dasjenige, das am meisten mit median_house_value korreliert ist? \n",
    "\n",
    "Was ist das am wenigsten korrelierte Feature? Was bedeutet dies? Und was bedeutet es nicht?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisieren Sie die Abhängigkeit von median_house_value und diesem wichtigsten Feature in einem Scatterplot in der nächsten Zelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was fällt auf und wie erklären Sie sich das? #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Schritt: Adding additional features\n",
    "\n",
    "Nachdem wir ein gutes Gefühl dafür haben, wie die Daten aussehen und welche Features relevant sind, nutzen wir dieses Wissen, um potentiell wichtige zusätzliche Features aus den bisher vorhandenen abzuleiten.  \n",
    "\n",
    "Das Datenset beinhaltet zum Beispiel die Features: \n",
    "- total_rooms: die Gesamtzahl von Räumen im District\n",
    "- total_bedrooms: die Gesamtzahl von Schlafzimmern im District\n",
    "- Population: die Anzahl von Einwohnern im District\n",
    "- Households: die Ahzahl von Haushalten im District\n",
    "\n",
    "Gestalten Sie aus diesen Features sinnvolle(re) neue Features, welche einen Einfluss auf den mittleren Preis eines Hauses haben könnten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnen Sie in der nächsten Zelle erneut die Korrelationmatrix und lassen sich die wichtigste Spalte nach Größe absteigend sortiert ausgeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Schritt: Preparing the data for ML: Train-Test Split und Input-Label Split\n",
    "\n",
    "Der dritte Schritt ist es, die Daten in Training Set und Test Set zu unterteilen. \n",
    "\n",
    "Hinweis: Dies müssen wir tun, BEVOR wir die Daten bereinigen. \n",
    "\n",
    "Grund: Wir wollen die Daten im Test set verwenden, um die Leistung des Algorithmus unvoreingenommen bewerten zu können (das heißt, dass der Algorithmus beim Trainieren keine Infos aus dem Test set gesehen haben darf). Aber beim Bereinigen der Daten werden oft Mittelwerte aus dem ganzen Datenset verwendet, um fehlende Daten zu ersetzen. Wenn dieser Mittelwert incl. der Testdaten berechnet wird, ist der Algorithmus schon voreingenommen bzgl. der Test Daten. Dies wäre ein Fall von **Data Leakage**.\n",
    "\n",
    "Nachdem wir separate Trainings und Test Sets haben, unterteilen wir diese in Input-Daten und Labels, weil ML Algorithmen diese als separaten Input brauchen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Test-Split ohne Scikit-Learn\n",
    "\n",
    "Um zu illustrieren, was bei diesem Split passiert, schreiben Sie eine Funktion \"split_train_test\", die \"housing\" in zwei DataFrames unterteilt. \n",
    "Argumente der Funktion: \n",
    "- ein DataFrame, der zu unterteilen ist\n",
    "- eine Zahl zwischen 0 und 1 (oft: 0.2), die angibt, welcher Bruchteil der Daten in das Test Set soll. \n",
    "Output der Funktion: \n",
    "Zwei DataFrames: training set und test set\n",
    "\n",
    "Hierbei ist wichtig, dass die housing daten vor dem Split zufällig durchpermutiert werden (verwenden Sie np.random.permutation auf die Zeilennummern), um irgendwelche \"Sortierungen\" in den Daten zu vermeiden.\n",
    "\n",
    "Um die Ergebnisse des zufälligen Durchpermutierens reproduzierbar zu machen für jeden Durchlauf in diesem Praktikum, setzen Sie zuvor np.random.seed(42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreiben Sie Ihre Funktion split_train_test hier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutzen Sie die Funktion, um die Daten 80-20 in \"train_set\" und \"test_set\" aufzuteilen\n",
    "\n",
    "\n",
    "# Lassen Sie sich die Länge der DataFrames ausgeben. Überprüfen Sie die Aufteilung 80-20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Test Split mit Scikit-Learn\n",
    "\n",
    "Nutzen Sie in der nächsten Zelle statt dessen die Scikit-Learn Methode [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), um die Daten in train_set und test_set aufzuteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwenden Sie Sklearn's train_test_split für den Split in train_set und test_set\n",
    "\n",
    "# Lassen Sie sich die ersten Zeilen von test_set ausgeben. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stratified Splits\n",
    "\n",
    "Obige Methoden unterteilen das Datenset zufällig in zwei Sets. Allerdings kann dies, wenn man Pech hat, zu einem sogenannten **Data mismatch** bzw. **nonrepresentative Training Data** führen. \n",
    "\n",
    "Was dies ist, lässt sich an einem einfachen Beispiel erklären: Angenommen, wir klassifizieren die MNIST-Bilder von handgeschriebenen Zahlen, und der Split unterteilt die Daten so, dass alle Bilder von Zahlen von 0-7 im Training Set und alle Bilder von 8 oder 9 im Test Set sind. Dann lernt der Algorithmus nur, die Zahlen 0-7 zu erkennen, und ist bei den Test-Bilder von 8 und 9 völlig aufgeschmissen! \n",
    "\n",
    "In unserem Beispiel könnte so etwas auch passiert sein: Angenommen, es sind zufällig alle Daten im Trainings-Set gelandet, die von \"reichen\" Districts kommen, mit hohem median_income - während alle ärmeren Districts im Test-Set landen. \n",
    "Dann wird das trainierte Modell ziemlich sicher falsche Prognosen generieren. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu sehen, ob dies bei unserem bisherigen train und test set der Fall ist, unterteilen wir das Feature median_income in fünf Einkommenskategorien und überprüfen, wie die Verteilung im train und test set ist im Vergleich zum gesamten Datenset. Führen Sie die vorbefüllten Zellen aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinzufügen einer neuen Feature-Spalte genannt \"income_cat\", \n",
    "# welche jede Instance (also jeden District) je nach Mittlerem Einkommen\n",
    "# in eine von 5 Einkommensklassen einteilt\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wie viel Prozent der Instances sind in welcher Einkommensklasse?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um auf jeden Fall **Data Mismatch** zu vermeiden, kann man einen **stratifizierten Split = stratified train-test-split** ausführen. \n",
    "Hierbei wird ein kategorielles Feature ausgewählt, von dem wir sicher gehen wollten, dass die einzelnen Klassen bei dem Split gleich verteilt werden (also die gleiche Verteilung haben wie im ursprünglichen Datenset). Falls das Feature nicht kategoriell ist, kann man es wie oben in ein kategorielles Feature verwandeln. \n",
    "\n",
    "(Vor allem bei Klassifizierungs-Tasks wie dem MNIST Beispiel von oben ist es hilfreich, einen stratifizierten Split bzgl der labels zu machen.)\n",
    "\n",
    "Im Folgenden lernen wir zwei Arten kennen, wie man einen stratified Split ausführen kann: \n",
    "\n",
    "Die einfachste: Scikit-Learn's train_test_split hat eine \"stratify\" Option. Hierzu verwenden Sie als Wert der stratify-Option die Spalte, bezüglich der stratifiziert werden soll. Modifizieren Sie den train_test_split Code von oben entsprechend in der nächsten Zelle und nennen Sie die entsprechenden Mengen \"strat_train_set\" und \"strat_test_set\". Alternativ können Sie auch [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) verwenden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifizierter train_test_split Code mit stratify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnen Sie die prozentuale Verteilung der 5 Einkommensklassen für train_set \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir income_cat für den stratifizierten Split verwendet haben, brauchen wir sie nicht mehr. Löschen Sie die Spalte wieder mit pandas' [drop()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) mit der Option \"inplace=True\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Löschen Sie die Spalte in allen DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input-Label Split\n",
    "\n",
    "Wir trennen nun sowohl für strat_train_set, als auch für strat_test_set die Labels von den Input-Daten.\n",
    "\n",
    "Löschen Sie hierzu mit .drop (dieses Mal mit Inplace=False) die label-Spalte \"median_house_value\" und nennen Sie den neuen DataFrame train.\n",
    "Kopieren Sie außerdem die Spalte median_house_value in einen DataFrame namens train_labels.\n",
    "\n",
    "Führen Sie dieselben Schritte durch für strat_test_set und nennen Sie die neuen DataFrames test und test_labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Speichern Sie die train_labels als csv-Datei \"train_labels.csv\" unter dem Pfad HOUSING_PATH ab. \n",
    "- Speichern Sie die test_labels als csv-Datei \"test_labels.csv\" unter dem Pfad HOUSING_PATH ab!\n",
    "\n",
    "ACHTUNG: Setzen Sie dabei immer index = False. Warum ist das wichtig, falls Sie später die Daten einlesen und zum Trainieren eines Modells verwenden wollen? \n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Schritt: Data Cleaning\n",
    "\n",
    "Als nächstes bereinigen wir die Trainings Daten (und nur diese), das heißt wir kümmern uns um fehlende Werte. Es gibt zwei Möglichkeiten, mit fehlenden Daten umzugehen: wir lassen sie entweder weg, oder ersetzen sie durch geeignete Werte (z.B. Durchschnittswerte). Doch zunächst müssen wir erst mal herausfinden, welche Werte fehlen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finden von fehlenden Werten in den Daten\n",
    "\n",
    "Finden Sie heraus, wie viele Werte in \"train\" in den einzelnen Features fehlen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir betrachten zunächst die Instances, die Null Values enthalten: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_rows = train[train.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Methoden, mit unvollständigen Daten umzugehen\n",
    "\n",
    "1. **Komplettes Feature löschen**: Wenn in einer Spalte sehr viele Werte nicht befüllt sind, kann es das beste sein, einfach die Spalte zu löschen und die Instances, die hier keinen Eintrag haben, zu behalten. (gut, wenn man wenige Daten hat und ein Feature systematisch schlecht befüllt ist)\n",
    "2. **Unvollständige Instances löschen**: Hiermit kann man alle Instances aus dem Datensatz löschen, in denen ein Datensatz fehlt. Über die Option subset kann man einschränken, in welcher Spalte hier nach fehlenden Werten gesucht wird. Diese Option ist besser, wenn es nur wenig Instances gibt, die schlecht sind, und nicht systematisch eine ganze Spalte schlecht ist.\n",
    "3. **Nichts löschen, sondern fehlende Werte ersetzen** mit .fillna(): generell eine bevorzugte Variante, weil man keine Instances oder Features verliert. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenden Sie 1. auf incomplete_rows an und sehen Sie sich das Ergebnis an. (mit dem default Inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenden Sie 2. auf incomplete_rows an und sehen Sie sich das Ergebnis an. (mit dem default Inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenden Sie 3. an und ersetzen die fehlenden Werte durch den Median mit fillna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unvollständige Daten ersetzen mit Scikit Learn\n",
    "\n",
    "Scikit Learn bietet eine bequeme Lösung, um den letzten Schritt (fehlende Werte durch irgendwelche passenden Werte ersetzen) automatisch für alle (numerischen) Spalten gleichzeitig auszuführen: [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n",
    "\n",
    "Legen Sie bitte zuerste eine Kopie \"housing_num\" des DataFrames ohne das nicht-numerische Feature \"ocean_proximity\" an, und wenden sie in der nächsten Zelle den SimpleImputer auf diese Kopie an mit .fit (berechnet die Zentralwerte) und .transform (ersetzt die fehlenden Daten mit den Zentralwerten). Das Ergebnis ist ein numpy ndarray. Verwandeln Sie es wieder in ein pandas Dataframe \"train_num_imputed\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Schritt: Verarzten von kategoriellen Features\n",
    "\n",
    "Ein ML Algorithmus kann keine Strings verarbeiten, deswegen müssen kategorielle Features wie \"ocean_proximity\" entweder in (1,2,3,...) oder in One-Hot Vektoren ((1,0,0,...), (0,1,0,0,....), (0,0,1,0,...)... statt \"<$1H OCEAN\"/\"INLAND\"/NEAR OCEAN...\")verwandelt werden. \n",
    "Dafür gibt es in sklearn den\n",
    "- [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)\n",
    "- [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "\n",
    "\n",
    "Was ist hier geeigneter und warum?\n",
    "\n",
    "#TODO\n",
    "\n",
    "\n",
    "Wenden Sie dies in der nächsten Zelle auf den Dataframe mit den kategoriellen Spalten (also hier nur: ocean_proximity - aber als Dataframe, nicht als Series!) an. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie verändert dies die Daten?\n",
    "\n",
    "#TODO\n",
    "\n",
    "Lassen Sie sich die Spaltenüberschriften ausgeben. \n",
    "Hinweis: benutzen Sie dafür das `.categories_` Attribut des oben benutzten Sklearn Estimators. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Schritt: Feature Scaling\n",
    "\n",
    "ML Algorithmen können in der Regel nicht gut mit unterschiedlich dimensionierten Features umgehen. Der Grund ist, dass größere Zahlen im direkten Vergleich mit sehr kleinen einfach mehr ins Gewicht fallen - das heißt, wenn ein wichtiges Feature sehr kleine Einträge hat, und ein unwichtiges s\n",
    "ehr große, dann kann es sein dass der Algorithmus trotzdem dem unwichtigen Feature mehr Gewicht gibt. \n",
    "\n",
    "Nennen Sie zwei Möglichkeiten, die Features zu skalieren, und überlegen Sie sich, welche Vor- und Nachteile es geben könnte!\n",
    "\n",
    "#TODO\n",
    "\n",
    "\n",
    "Verwenden Sie in der nächsten Zelle den [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), um die numerischen Features train_num_imputed zu standardisieren. Er gibt ein ndarray zurück. Wandeln Sie es wieder in ein pandas DataFrame um und lassen Sie sich die ersten Zeilen ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verwenden einer Pipeline für das Data Processing \n",
    "\n",
    "Die obigen Schritte für die numerischen Spalten, also Data Cleaning und dann Standardisieren, kann man auch in einer Pipeline auf einmal machen. \n",
    "\n",
    "Wenden Sie dafür Scikit Learn's [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), um den SimpleImputer und den StandardScaler in einem Schritt auf die untransformierten numerischen Daten housing_num anzuwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir allerdings nicht nur die numerischen, sondern die numerischen und die kategoriellen Features gleichzeitig in einer Pipeline transformieren wollen, geht das mit der [ColumnTransformer] (https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html). \n",
    "\n",
    "Verwenden Sie den ColumnTransformer, um die oben durchgeführten Transformationen (die numerische Pipeline für die numerischen Spalten, und OneHotEncoder für die kategorielle Spalte ocean_proximity) auf den ursprünglichen DataFrame training anzuwenden. \n",
    "\n",
    "Das Resultat des Transformers ist wieder ein ndarray. Verwandeln Sie es zu Übung wieder in einen DataFrame, aber Achtung: Wenn Sie das Ergebnis in einen DataFrame umwandeln, beachten Sie, dass die Spaltenüberschriften nun andere sind! Zusätzlich zu den numerischen Features gibt es nun für jede ocean_proximity-Kategorie eine Spalte!\n",
    "\n",
    "Nennen Sie das resultierende DataFrame housing_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenden Sie den ColumnTransformer auch auf das Test Set an, und nennen Sie das resultierende DataFrame housing_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichern Sie die Trainings-Datensätze housing_train als csv-Datei \"housing_train.csv\" und housing_test als csv-Datei \"housing_test.csv\" unter dem Pfad HOUSING_PATH ab! Setzen Sie dabei index = False wie zuvor (aus den gleichen Gründen!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zählen Sie hier noch einmal auf wie Sie allgemein vorgehen würden für Schritte 4-6 bei einem Datensatz mit numerischen und kategoriellen Features\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainieren eines LinearRegression Models und Evaluation\n",
    "\n",
    "Trainieren Sie ein [Lineares Regressionsmodell](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) lin_reg. \n",
    "\n",
    "Dazu generieren Sie zuerst ein Objekt lin_reg der Klasse LinearRegression, und trainieren Sie dieses Objekt dann mit `lin_reg.fit(housing_train, train_labels)``. Dadurch werden die Parameter des Modells den Daten angepasst, so dass das Modell die `train_labels`` möglichst genau vorhersagt.\n",
    "\n",
    "Lassen Sie sich dann den Score ausgeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lassen Sie sich die trainierten Parameter ausgeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geben Sie vier Performance Measures für Regression an, und beschreiben Sie in jeweils einem Satz, was diese messen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generieren Sie die Vorhersagen des Modells auf den Testdaten mit `predict()``. \n",
    "- Lassen Sie sich alle vier performance measures auf den Testdaten ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ist dieses Ergebnis gut? Wenn nein, was könnte der Grund sein?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beschreiben Sie im nächsten Markdown-Fenster: \n",
    "- wie Cross Validation funktioniert\n",
    "- was der Vorteil von Cross-Validation ist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnen Sie die 10-fold Cross-Validation Scores des linearen Modells auf dem training set, wobei Sie als zugrunde liegende performance measure den negativen MSE verwenden. Geben Sie die Scores aus, zusammen mit ihrem Mittelwert und der Standardabweichung. (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
