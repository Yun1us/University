{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CS 4476 Project 2: Local Feature Matching](https://dellaert.github.io/19F-4476/proj2.html)\n",
    "\n",
    "This iPython notebook:  \n",
    "(1) Loads and resizes images  \n",
    "(2) Finds interest points in those images                 (you code this)  \n",
    "(3) Describes each interest point with a local feature    (you code this)  \n",
    "(4) Finds matching features                               (you code this)  \n",
    "(5) Visualizes the matches  \n",
    "(6) Evaluates the matches based on ground truth correspondences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import load_image, PIL_resize, rgb2gray\n",
    "from IPython.core.debugger import set_trace\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Notre Dame\n",
    "image1 = load_image('../data/1a_notredame.jpg')\n",
    "image2 = load_image('../data/1b_notredame.jpg')\n",
    "eval_file = '../ground_truth/notredame.pkl'\n",
    "\n",
    "# # Mount Rushmore -- this pair is relatively easy (still harder than Notre Dame, though)\n",
    "# image1 = load_image('../data/2a_rushmore.jpg')\n",
    "# image2 = load_image('../data/2b_rushmore.jpg')\n",
    "# eval_file = '../ground_truth/rushmore.pkl'\n",
    "\n",
    "# # Episcopal Gaudi -- This pair is relatively difficult\n",
    "# image1 = load_image('../data/3a_gaudi.jpg')\n",
    "# image2 = load_image('../data/3b_gaudi.jpg')\n",
    "# eval_file = '../ground_truth/gaudi.pkl'\n",
    "\n",
    "scale_factor = 0.5\n",
    "image1 = PIL_resize(image1, (int(image1.shape[1]*scale_factor), int(image1.shape[0]*scale_factor)))\n",
    "image2 = PIL_resize(image2, (int(image2.shape[1]*scale_factor), int(image2.shape[0]*scale_factor)))\n",
    "\n",
    "image1_bw = rgb2gray(image1)\n",
    "image2_bw = rgb2gray(image2)\n",
    "\n",
    "#convert images to tensor\n",
    "tensor_type = torch.FloatTensor\n",
    "torch.set_default_tensor_type(tensor_type)\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "image_input1 = to_tensor(image1_bw).unsqueeze(0)\n",
    "image_input2 = to_tensor(image2_bw).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1: Harris Corner Detector \n",
    "## Find distinctive points in each image (Szeliski 4.1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (HarrisNet.py, line 249)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/proj/mari/marim/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3550\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[3], line 6\u001b[0m\n    from unit_test_harris import (\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/proj/vl/ki310cv/labs/lab02/code/unit_test_harris.py:5\u001b[0;36m\n\u001b[0;31m    from HarrisNet import (\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/proj/vl/ki310cv/labs/lab02/code/HarrisNet.py:249\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise NotImplementedError('`CornerResponseLayer` needs to be ''\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "## Verify each layer in the code, this will check if your implementation is correct or not.\n",
    "\n",
    "## Do not modify the constructor of any layer (i.e. to take some custom arguments\n",
    "## as input)\n",
    "\n",
    "from unit_test_harris import (\n",
    "    test_ImageGradientsLayer,\n",
    "    test_ChannelProductLayer, \n",
    "    test_SecondMomentMatrixLayer, \n",
    "    test_CornerResponseLayer, \n",
    "    test_NMSLayer,\n",
    "    verify\n",
    ")\n",
    "\n",
    "print('ImageGradientsLayer:', verify(test_ImageGradientsLayer))\n",
    "print('ChannelProductLayer:', verify(test_ChannelProductLayer))\n",
    "print('SecondMomentMatrixLayer:', verify(test_SecondMomentMatrixLayer))\n",
    "print('CornerResponseLayer:', verify(test_CornerResponseLayer) )\n",
    "print('NMSLayer:', verify(test_NMSLayer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HarrisNet import get_interest_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will call get_interest_points function in HarrisNet.py to detect 'interesting' points in the images. \n",
    "\n",
    "**IMPORTANT**\n",
    "Make sure to add your code in get_interest_points function to call Harris Corner Detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_interest_points\n",
    "\n",
    "x1, y1, _ = get_interest_points(image_input1)\n",
    "x2, y2, _ = get_interest_points(image_input2)\n",
    "\n",
    "x1, x2 = x1.detach().numpy(), x2.detach().numpy()\n",
    "y1, y2 = y1.detach().numpy(), y2.detach().numpy()\n",
    "\n",
    "# Visualize the interest points\n",
    "c1 = show_interest_points(image1, x1, y1)\n",
    "c2 = show_interest_points(image2, x2, y2)\n",
    "plt.figure(); plt.imshow(c1)\n",
    "plt.figure(); plt.imshow(c2)\n",
    "print('{:d} corners in image 1, {:d} corners in image 2'.format(len(x1), len(x2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sift Feature Descriptor\n",
    "## Create feature vectors at each interest point (Szeliski 4.1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSIFTNet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     angles_to_vectors_2d_pytorch,\n\u001b[1;32m      3\u001b[0m     HistogramLayer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     get_siftnet_features\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_layer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageGradientsLayer\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munit_test_sift\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     test_angles_to_vectors_2d_pytorch,\n\u001b[1;32m     12\u001b[0m     test_HistogramLayer,\n\u001b[1;32m     13\u001b[0m     test_ImageGradientsLayer,\n\u001b[1;32m     14\u001b[0m     test_SubGridAccumulationLayer,\n\u001b[1;32m     15\u001b[0m     test_SIFTOrientationLayer,\n\u001b[1;32m     16\u001b[0m     test_get_sift_subgrid_coords,\n\u001b[1;32m     17\u001b[0m     test_SIFTNet,\n\u001b[1;32m     18\u001b[0m     test_get_siftnet_features\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mangles_to_vectors_2d_pytorch:\u001b[39m\u001b[38;5;124m'\u001b[39m, verify(test_angles_to_vectors_2d_pytorch))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHistogramLayer:\u001b[39m\u001b[38;5;124m'\u001b[39m, verify(test_HistogramLayer))\n",
      "File \u001b[0;32m~/proj/vl/ki310cv/labs/lab02/code/unit_test_sift.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mproj2_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSIFTNet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m \tHistogramLayer,\n\u001b[1;32m     13\u001b[0m \tImageGradientsLayer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \tget_siftnet_features\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mproj2_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     load_image,\n\u001b[1;32m     23\u001b[0m     evaluate_correspondence,\n\u001b[1;32m     24\u001b[0m     rgb2gray, \n\u001b[1;32m     25\u001b[0m     PIL_resize\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from SIFTNet import (\n",
    "    angles_to_vectors_2d_pytorch,\n",
    "    HistogramLayer,\n",
    "    SubGridAccumulationLayer,\n",
    "    SIFTOrientationLayer,\n",
    "    get_sift_subgrid_coords,\n",
    "    get_siftnet_features\n",
    ")\n",
    "from torch_layer_utils import ImageGradientsLayer\n",
    "from unit_test_sift import (\n",
    "    test_angles_to_vectors_2d_pytorch,\n",
    "    test_HistogramLayer,\n",
    "    test_ImageGradientsLayer,\n",
    "    test_SubGridAccumulationLayer,\n",
    "    test_SIFTOrientationLayer,\n",
    "    test_get_sift_subgrid_coords,\n",
    "    test_SIFTNet,\n",
    "    test_get_siftnet_features\n",
    ")\n",
    "\n",
    "print('angles_to_vectors_2d_pytorch:', verify(test_angles_to_vectors_2d_pytorch))\n",
    "print('HistogramLayer:', verify(test_HistogramLayer))\n",
    "print('ImageGradientsLayer:', verify(test_ImageGradientsLayer))\n",
    "print('SIFTOrientationLayer:', verify(test_SIFTOrientationLayer) )\n",
    "print('SIFTNet:', verify(test_SIFTNet) )\n",
    "print('SubGridAccumulationLayer:', verify(test_SubGridAccumulationLayer))\n",
    "print('get_sift_subgrid_coords:', verify(test_get_sift_subgrid_coords) )\n",
    "print('get_siftnet_features:', verify(test_get_siftnet_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image1_features = get_siftnet_features(image_input1, x1, y1)\n",
    "image2_features = get_siftnet_features(image_input2, x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match features (Szeliski 4.1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'proj2_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#test your feature matching implementation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munit_test_matching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test_feature_matching, test_compute_dists\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompute_dists:\u001b[39m\u001b[38;5;124m'\u001b[39m, verify(test_compute_dists))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_matching:\u001b[39m\u001b[38;5;124m'\u001b[39m, verify(test_feature_matching))\n",
      "File \u001b[0;32m~/proj/vl/ki310cv/labs/lab02/code/unit_test_matching.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mproj2_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudent_feature_matching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m match_features, compute_feature_distances\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_compute_dists\u001b[39m():\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Test feature distance calculations.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'proj2_code'"
     ]
    }
   ],
   "source": [
    "#test your feature matching implementation\n",
    "from unit_test_matching import test_feature_matching, test_compute_dists\n",
    "print('compute_dists:', verify(test_compute_dists))\n",
    "print('feature_matching:', verify(test_feature_matching))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from student_feature_matching import match_features\n",
    "matches, confidences = match_features(image1_features, image2_features, x1, y1, x2, y2)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(x1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "You might want to set 'num_pts_to_visualize' and 'num_pts_to_evaluate' to some constant (e.g. 100) once you start detecting hundreds of interest points, otherwise things might get too cluttered. You could also threshold based on confidence.  \n",
    "  \n",
    "There are two visualization functions below. You can comment out one of both of them if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_correspondence_circles, show_correspondence_lines\n",
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 100\n",
    "c1 = show_correspondence_circles(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c1)\n",
    "plt.savefig('../results/vis_circles.jpg', dpi=1000)\n",
    "c2 = show_correspondence_lines(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c2)\n",
    "plt.savefig('../results/vis_lines.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out the function below if you are not testing on the Notre Dame, Episcopal Gaudi, and Mount Rushmore image pairs--this evaluation function will only work for those which have ground truth available.  \n",
    "  \n",
    "You can use `annotate_correspondences/collect_ground_truth_corr.py` to build the ground truth for other image pairs if you want, but it's very tedious. It would be a great service to the class for future years, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_correspondence\n",
    "# num_pts_to_evaluate = len(matches)\n",
    "num_pts_to_evaluate = 100\n",
    "_, c = evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "                        x1[matches[:num_pts_to_evaluate, 0]], y1[matches[:num_pts_to_evaluate, 0]],\n",
    "                        x2[matches[:num_pts_to_evaluate, 1]], y2[matches[:num_pts_to_evaluate, 1]])\n",
    "plt.figure(); plt.imshow(c)\n",
    "plt.savefig('../results/eval.jpg', dpi=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
